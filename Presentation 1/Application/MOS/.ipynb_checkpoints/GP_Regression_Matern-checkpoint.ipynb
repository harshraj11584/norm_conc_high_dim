{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt \n",
    "# from scipy import stats\n",
    "# from scipy.special import boxcox, inv_boxcox\n",
    "from tqdm import tqdm \n",
    "# import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import Pool \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 100)\n",
      "(900, 100) (60, 100) (240, 100)\n",
      "(900,) (60,) (240,)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('MOS_Scores.mat', 'r') as f:\n",
    "\tfor k,v in f.items():\n",
    "\t\tY = (np.array(v))\n",
    "\t\tY = Y.reshape(Y.shape[1])\n",
    "\t\tnp.save('Y_mat.npy',Y)\n",
    "with h5py.File('konvid_features.mat', 'r') as f:\n",
    "\tfor k,v in f.items():\n",
    "\t\tX = (np.array(v)).T\n",
    "\t\tnp.save('X_mat.npy',X)\n",
    "\n",
    "\t\tprint(X.shape)\n",
    "\n",
    "\t\t# X1 = (X**2).copy()\n",
    "\t\t# X = np.hstack((X,X1))\n",
    "\t\t\n",
    "\t\tX = StandardScaler().fit_transform(X)\n",
    "        \n",
    "x_train,x_test,y_train,y_test = sklearn.model_selection.train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=42)\n",
    "x_train,x_val,y_train,y_val = x_train[:900].copy(), x_train[900:].copy(),y_train[:900].copy(),y_train[900:].copy()\n",
    "\n",
    "print(x_train.shape,x_val.shape,x_test.shape)\n",
    "print(y_train.shape,y_val.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Neural Net Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(25, input_dim=100, kernel_regularizer=regularizers.l2(0.01),activation='elu'))\n",
    "# model.add(Dense(20, kernel_regularizer=regularizers.l2(0.01),activation='elu'))\n",
    "# model.add(Dense(15, kernel_regularizer=regularizers.l2(0.01),activation='elu'))\n",
    "# model.add(Dense(10,kernel_regularizer=regularizers.l2(0.01),activation='elu'))\n",
    "# model.add(Dense(5,activation='elu'))\n",
    "# model.add(Dense(1, activation='elu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train, y_train, epochs=50, validation_data=(x_val,y_val),batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(x_test)\n",
    "# y_pred = y_pred.reshape(y_pred.shape[0])\n",
    "# print(y_pred.shape,y_test.shape)\n",
    "# print(np.corrcoef(y_test,y_pred)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp_cols = np.array([2,9,17,23,27,30,41,47,50,51,52,54,57,59,65,67,68,70,72,74,75,78,83,85,90,91,99,100,101])-2\n",
    "# X1 = np.hstack((X[:,imp_cols],X[:,imp_cols]**2,X[:,imp_cols]**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_pred[:10])\n",
    "# print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(y_pred[:10])\n",
    "# plt.plot(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(y_pred-y_test,bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([900, 100]) torch.Size([900]) torch.Size([300, 100]) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "train_x = torch.tensor(x_train.astype(np.float32))\n",
    "train_y = torch.tensor(y_train.astype(np.float32))\n",
    "test_x = torch.tensor(np.concatenate((x_val,x_test)).astype(np.float32))\n",
    "test_y = torch.tensor(np.concatenate((y_val,y_test)).astype(np.float32))\n",
    "print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "#         self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 0.695\n",
      "Iter 2/100 - Loss: 0.697\n",
      "Iter 3/100 - Loss: 0.691\n",
      "Iter 4/100 - Loss: 0.695\n",
      "Iter 5/100 - Loss: 0.696\n",
      "Iter 6/100 - Loss: 0.694\n",
      "Iter 7/100 - Loss: 0.691\n",
      "Iter 8/100 - Loss: 0.691\n",
      "Iter 9/100 - Loss: 0.700\n",
      "Iter 10/100 - Loss: 0.702\n",
      "Iter 11/100 - Loss: 0.692\n",
      "Iter 12/100 - Loss: 0.693\n",
      "Iter 13/100 - Loss: 0.694\n",
      "Iter 14/100 - Loss: 0.700\n",
      "Iter 15/100 - Loss: 0.696\n",
      "Iter 16/100 - Loss: 0.696\n",
      "Iter 17/100 - Loss: 0.688\n",
      "Iter 18/100 - Loss: 0.694\n",
      "Iter 19/100 - Loss: 0.688\n",
      "Iter 20/100 - Loss: 0.690\n",
      "Iter 21/100 - Loss: 0.690\n",
      "Iter 22/100 - Loss: 0.694\n",
      "Iter 23/100 - Loss: 0.691\n",
      "Iter 24/100 - Loss: 0.696\n",
      "Iter 25/100 - Loss: 0.691\n",
      "Iter 26/100 - Loss: 0.696\n",
      "Iter 27/100 - Loss: 0.688\n",
      "Iter 28/100 - Loss: 0.692\n",
      "Iter 29/100 - Loss: 0.690\n",
      "Iter 30/100 - Loss: 0.683\n",
      "Iter 31/100 - Loss: 0.697\n",
      "Iter 32/100 - Loss: 0.691\n",
      "Iter 33/100 - Loss: 0.691\n",
      "Iter 34/100 - Loss: 0.695\n",
      "Iter 35/100 - Loss: 0.699\n",
      "Iter 36/100 - Loss: 0.695\n",
      "Iter 37/100 - Loss: 0.698\n",
      "Iter 38/100 - Loss: 0.695\n",
      "Iter 39/100 - Loss: 0.691\n",
      "Iter 40/100 - Loss: 0.696\n",
      "Iter 41/100 - Loss: 0.691\n",
      "Iter 42/100 - Loss: 0.697\n",
      "Iter 43/100 - Loss: 0.692\n",
      "Iter 44/100 - Loss: 0.691\n",
      "Iter 45/100 - Loss: 0.694\n",
      "Iter 46/100 - Loss: 0.696\n",
      "Iter 47/100 - Loss: 0.689\n",
      "Iter 48/100 - Loss: 0.692\n",
      "Iter 49/100 - Loss: 0.691\n",
      "Iter 50/100 - Loss: 0.687\n",
      "Iter 51/100 - Loss: 0.699\n",
      "Iter 52/100 - Loss: 0.692\n",
      "Iter 53/100 - Loss: 0.697\n",
      "Iter 54/100 - Loss: 0.696\n",
      "Iter 55/100 - Loss: 0.693\n",
      "Iter 56/100 - Loss: 0.697\n",
      "Iter 57/100 - Loss: 0.697\n",
      "Iter 58/100 - Loss: 0.694\n",
      "Iter 59/100 - Loss: 0.695\n",
      "Iter 60/100 - Loss: 0.699\n",
      "Iter 61/100 - Loss: 0.695\n",
      "Iter 62/100 - Loss: 0.692\n",
      "Iter 63/100 - Loss: 0.694\n",
      "Iter 64/100 - Loss: 0.696\n",
      "Iter 65/100 - Loss: 0.686\n",
      "Iter 66/100 - Loss: 0.697\n",
      "Iter 67/100 - Loss: 0.691\n",
      "Iter 68/100 - Loss: 0.693\n",
      "Iter 69/100 - Loss: 0.695\n",
      "Iter 70/100 - Loss: 0.693\n",
      "Iter 71/100 - Loss: 0.687\n",
      "Iter 72/100 - Loss: 0.696\n",
      "Iter 73/100 - Loss: 0.697\n",
      "Iter 74/100 - Loss: 0.691\n",
      "Iter 75/100 - Loss: 0.698\n",
      "Iter 76/100 - Loss: 0.698\n",
      "Iter 77/100 - Loss: 0.691\n",
      "Iter 78/100 - Loss: 0.692\n",
      "Iter 79/100 - Loss: 0.699\n",
      "Iter 80/100 - Loss: 0.690\n",
      "Iter 81/100 - Loss: 0.695\n",
      "Iter 82/100 - Loss: 0.695\n",
      "Iter 83/100 - Loss: 0.697\n",
      "Iter 84/100 - Loss: 0.694\n",
      "Iter 85/100 - Loss: 0.691\n",
      "Iter 86/100 - Loss: 0.694\n",
      "Iter 87/100 - Loss: 0.693\n",
      "Iter 88/100 - Loss: 0.689\n",
      "Iter 89/100 - Loss: 0.692\n",
      "Iter 90/100 - Loss: 0.696\n",
      "Iter 91/100 - Loss: 0.690\n",
      "Iter 92/100 - Loss: 0.697\n",
      "Iter 93/100 - Loss: 0.694\n",
      "Iter 94/100 - Loss: 0.691\n",
      "Iter 95/100 - Loss: 0.694\n",
      "Iter 96/100 - Loss: 0.693\n",
      "Iter 97/100 - Loss: 0.699\n",
      "Iter 98/100 - Loss: 0.691\n",
      "Iter 99/100 - Loss: 0.694\n",
      "Iter 100/100 - Loss: 0.699\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()},],lr=0.001)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iters = 100\n",
    "for i in range(training_iters):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (\n",
    "        i + 1, training_iters, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300]) (240,)\n",
      "pearson 0.6960053810683573\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "f_preds = model(test_x)\n",
    "y_pred = f_preds.mean\n",
    "\n",
    "print(y_pred.shape,y_test.shape)\n",
    "print(\"pearson\",np.corrcoef(test_y.detach().numpy(),y_pred.detach().numpy())[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman 0.691972909510844\n"
     ]
    }
   ],
   "source": [
    "a=test_y.detach().numpy()\n",
    "b=y_pred.detach().numpy()\n",
    "import scipy.stats\n",
    "print(\"Spearman\",scipy.stats.spearmanr(a, b, axis=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
