{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt \n",
    "# from scipy import stats\n",
    "# from scipy.special import boxcox, inv_boxcox\n",
    "from tqdm import tqdm \n",
    "# import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import Pool \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 100)\n",
      "(900, 100) (60, 100) (240, 100)\n",
      "(900,) (60,) (240,)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('MOS_Scores.mat', 'r') as f:\n",
    "    for k,v in f.items():\n",
    "        Y = (np.array(v))\n",
    "        Y = Y.reshape(Y.shape[1])\n",
    "        np.save('Y_mat.npy',Y)\n",
    "with h5py.File('konvid_features.mat', 'r') as f:\n",
    "    for k,v in f.items():\n",
    "        X = (np.array(v)).T\n",
    "        np.save('X_mat.npy',X)\n",
    "\n",
    "        \n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        imp_cols = np.array([2,9,17,23,27,30,41,47,50,51,52,54,57,59,65,67,68,70,72,74,75,78,83,85,90,91,99,100,101])-2\n",
    "#         X = X[:,imp_cols]\n",
    "        \n",
    "        print(X.shape)\n",
    "        \n",
    "x_train,x_test,y_train,y_test = sklearn.model_selection.train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=42)\n",
    "x_train,x_val,y_train,y_val = x_train[:900].copy(), x_train[900:].copy(),y_train[:900].copy(),y_train[900:].copy()\n",
    "\n",
    "print(x_train.shape,x_val.shape,x_test.shape)\n",
    "print(y_train.shape,y_val.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_cols = np.array([2,9,17,23,27,30,41,47,50,51,52,54,57,59,65,67,68,70,72,74,75,78,83,85,90,91,99,100,101])-2\n",
    "rem_cols = np.setdiff1d(np.arange(100),imp_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Neural Net Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(25, input_dim=100, kernel_regularizer=regularizers.l2(0.01),activation='elu'))\n",
    "# model.add(Dense(20, kernel_regularizer=regularizers.l2(0.01),activation='elu'))\n",
    "# model.add(Dense(15, kernel_regularizer=regularizers.l2(0.01),activation='elu'))\n",
    "# model.add(Dense(10,kernel_regularizer=regularizers.l2(0.01),activation='elu'))\n",
    "# model.add(Dense(5,activation='elu'))\n",
    "# model.add(Dense(1, activation='elu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train, y_train, epochs=50, validation_data=(x_val,y_val),batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(x_test)\n",
    "# y_pred = y_pred.reshape(y_pred.shape[0])\n",
    "# print(y_pred.shape,y_test.shape)\n",
    "# print(np.corrcoef(y_test,y_pred)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp_cols = np.array([2,9,17,23,27,30,41,47,50,51,52,54,57,59,65,67,68,70,72,74,75,78,83,85,90,91,99,100,101])-2\n",
    "# X1 = np.hstack((X[:,imp_cols],X[:,imp_cols]**2,X[:,imp_cols]**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_pred[:10])\n",
    "# print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(y_pred[:10])\n",
    "# plt.plot(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(y_pred-y_test,bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([900, 100]) torch.Size([900]) torch.Size([300, 100]) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "train_x = torch.tensor(x_train.astype(np.float32))\n",
    "train_y = torch.tensor(y_train.astype(np.float32))\n",
    "test_x = torch.tensor(np.concatenate((x_val,x_test)).astype(np.float32))\n",
    "test_y = torch.tensor(np.concatenate((y_val,y_test)).astype(np.float32))\n",
    "print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "#         self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dimns=x_train.shape[1]))\n",
    "#         self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5,ard_num_dimns=x_train.shape[1]) + gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5,ard_num_dims=len(imp_cols),active_dims=imp_cols) * gpytorch.kernels.MaternKernel(active_dims=rem_cols))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 0.707\n",
      "Iter 2/50 - Loss: 0.701\n",
      "Iter 3/50 - Loss: 0.704\n",
      "Iter 4/50 - Loss: 0.699\n",
      "Iter 5/50 - Loss: 0.705\n",
      "Iter 6/50 - Loss: 0.704\n",
      "Iter 7/50 - Loss: 0.700\n",
      "Iter 8/50 - Loss: 0.703\n",
      "Iter 9/50 - Loss: 0.700\n",
      "Iter 10/50 - Loss: 0.714\n",
      "Iter 11/50 - Loss: 0.706\n",
      "Iter 12/50 - Loss: 0.705\n",
      "Iter 13/50 - Loss: 0.703\n",
      "Iter 14/50 - Loss: 0.704\n",
      "Iter 15/50 - Loss: 0.712\n",
      "Iter 16/50 - Loss: 0.709\n",
      "Iter 17/50 - Loss: 0.705\n",
      "Iter 18/50 - Loss: 0.700\n",
      "Iter 19/50 - Loss: 0.709\n",
      "Iter 20/50 - Loss: 0.704\n",
      "Iter 21/50 - Loss: 0.700\n",
      "Iter 22/50 - Loss: 0.703\n",
      "Iter 23/50 - Loss: 0.710\n",
      "Iter 24/50 - Loss: 0.699\n",
      "Iter 25/50 - Loss: 0.698\n",
      "Iter 26/50 - Loss: 0.703\n",
      "Iter 27/50 - Loss: 0.708\n",
      "Iter 28/50 - Loss: 0.711\n",
      "Iter 29/50 - Loss: 0.705\n",
      "Iter 30/50 - Loss: 0.700\n",
      "Iter 31/50 - Loss: 0.710\n",
      "Iter 32/50 - Loss: 0.701\n",
      "Iter 33/50 - Loss: 0.706\n",
      "Iter 34/50 - Loss: 0.708\n",
      "Iter 35/50 - Loss: 0.701\n",
      "Iter 36/50 - Loss: 0.713\n",
      "Iter 37/50 - Loss: 0.705\n",
      "Iter 38/50 - Loss: 0.701\n",
      "Iter 39/50 - Loss: 0.708\n",
      "Iter 40/50 - Loss: 0.716\n",
      "Iter 41/50 - Loss: 0.700\n",
      "Iter 42/50 - Loss: 0.712\n",
      "Iter 43/50 - Loss: 0.705\n",
      "Iter 44/50 - Loss: 0.707\n",
      "Iter 45/50 - Loss: 0.704\n",
      "Iter 46/50 - Loss: 0.705\n",
      "Iter 47/50 - Loss: 0.703\n",
      "Iter 48/50 - Loss: 0.703\n",
      "Iter 49/50 - Loss: 0.700\n",
      "Iter 50/50 - Loss: 0.707\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()},],lr=0.001)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iters = 50\n",
    "for i in range(training_iters):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (\n",
    "            i + 1, training_iters, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise           value = -1.6555365324020386\n",
      "Parameter name: covar_module.raw_outputscale               value = 1.9082797765731812\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6714299c0c2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Parameter name: {param_name:42} value = {param.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300]) (240,)\n",
      "pearson 0.6894163204202093\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "f_preds = model(test_x)\n",
    "y_pred = f_preds.mean\n",
    "\n",
    "print(y_pred.shape,y_test.shape)\n",
    "print(\"pearson\",np.corrcoef(test_y.detach().numpy(),y_pred.detach().numpy())[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman 0.6897298990832388\n"
     ]
    }
   ],
   "source": [
    "a=test_y.detach().numpy()\n",
    "b=y_pred.detach().numpy()\n",
    "import scipy.stats\n",
    "print(\"Spearman\",scipy.stats.spearmanr(a, b, axis=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
